{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:\\AI\\Kì 5\\DeepLearning\\Proj\\LLaMA-Omni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'LLaMA-Omni'...\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ictnlp/LLaMA-Omni\n",
    "%cd LLaMA-Omni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install [options] <requirement specifier> [package-index-options] ...\n",
      "  c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install [options] -r <requirements file> [package-index-options] ...\n",
      "  c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install [options] [-e] <vcs project url> ...\n",
      "  c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install [options] [-e] <local project path> ...\n",
      "  c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install [options] <archive url/path> ...\n",
      "\n",
      "no such option: -n\n"
     ]
    }
   ],
   "source": [
    "%pip install conda create -n myenv python=3.11.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'conda' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip==24.0\n",
      "  Using cached pip-24.0-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\python.exe -m pip install pip==24.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///F:/AI/K%C3%AC%205/DeepLearning/Proj/LLaMA-Omni\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Getting requirements to build editable: started\n",
      "  Getting requirements to build editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting torch==2.1.2 (from llama-omni==1.0.0)\n",
      "  Using cached torch-2.1.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting torchvision==0.16.2 (from llama-omni==1.0.0)\n",
      "  Using cached torchvision-0.16.2-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio==2.1.2 (from llama-omni==1.0.0)\n",
      "  Using cached torchaudio-2.1.2-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting transformers==4.43.4 (from llama-omni==1.0.0)\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tokenizers==0.19.1 (from llama-omni==1.0.0)\n",
      "  Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting sentencepiece==0.1.99 (from llama-omni==1.0.0)\n",
      "  Using cached sentencepiece-0.1.99-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting shortuuid (from llama-omni==1.0.0)\n",
      "  Using cached shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting accelerate==0.33.0 (from llama-omni==1.0.0)\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting peft==0.11.1 (from llama-omni==1.0.0)\n",
      "  Using cached peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes==0.43.1 (from llama-omni==1.0.0)\n",
      "  Using cached bitsandbytes-0.43.1-py3-none-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pydantic in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-omni==1.0.0) (2.7.4)\n",
      "Collecting markdown2[all] (from llama-omni==1.0.0)\n",
      "  Using cached markdown2-2.5.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-omni==1.0.0) (1.26.3)\n",
      "Collecting scikit-learn==1.2.2 (from llama-omni==1.0.0)\n",
      "  Using cached scikit_learn-1.2.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting gradio==4.43.0 (from llama-omni==1.0.0)\n",
      "  Using cached gradio-4.43.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==1.3.0 (from llama-omni==1.0.0)\n",
      "  Using cached gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-omni==1.0.0) (2.32.3)\n",
      "Collecting httpx==0.27.2 (from llama-omni==1.0.0)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting uvicorn (from llama-omni==1.0.0)\n",
      "  Using cached uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastapi (from llama-omni==1.0.0)\n",
      "  Using cached fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting soundfile (from llama-omni==1.0.0)\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl.metadata (14 kB)\n",
      "Collecting einops==0.6.1 (from llama-omni==1.0.0)\n",
      "  Using cached einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting einops-exts==0.0.4 (from llama-omni==1.0.0)\n",
      "  Using cached einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting timm==0.6.13 (from llama-omni==1.0.0)\n",
      "  Using cached timm-0.6.13-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting openai-whisper (from llama-omni==1.0.0)\n",
      "  Using cached openai-whisper-20240930.tar.gz (800 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting setuptools==59.5.0 (from llama-omni==1.0.0)\n",
      "  Using cached setuptools-59.5.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting omegaconf==2.0.6 (from llama-omni==1.0.0)\n",
      "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of llama-omni to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "  WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf==2.0.6 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from llama-omni==1.0.0) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "ERROR: Ignored the following yanked versions: 1.0.0, 1.0.1, 1.0.2, 2.0.0rc1, 2.0.0rc2, 2.0.0rc22, 2.0.0rc23, 2.0.0rc24, 2.0.0rc25, 2.0.0rc26, 2.0.0rc27, 2.0.0rc28, 2.0.0rc29, 2.0.1rc1, 2.0.1rc2, 2.0.1rc3, 2.0.1rc4, 2.0.1rc5, 2.2.0\n",
      "ERROR: Could not find a version that satisfies the requirement omegaconf==2.0.6 (from llama-omni) (from versions: 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.0.9, 1.0.10, 1.0.11, 1.0.12, 1.0.13, 1.0.14, 1.0.16, 1.0.17, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.1.9, 1.1.10, 1.2.0, 1.2.1, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0rc4, 1.3.0rc5, 1.3.0rc6, 1.3.0rc7, 1.3.0rc8, 1.3.0rc9, 1.3.0rc10, 1.3.0, 1.4.0rc1, 1.4.0rc2, 1.4.0rc3, 1.4.0rc4, 1.4.0, 1.4.1rc1, 1.4.1, 2.0.0rc3, 2.0.0rc4, 2.0.0rc5, 2.0.0rc6, 2.0.0rc7, 2.0.0rc8, 2.0.0rc9, 2.0.0rc10, 2.0.0rc11, 2.0.0rc12, 2.0.0rc13, 2.0.0rc14, 2.0.0rc15, 2.0.0rc16, 2.0.0rc17, 2.0.0rc18, 2.0.0rc19, 2.0.0rc20, 2.0.0rc21, 2.0.0, 2.0.1rc6, 2.0.1rc7, 2.0.1rc8, 2.0.1rc9, 2.0.1rc10, 2.0.1rc11, 2.0.1rc12, 2.0.1rc13, 2.0.1, 2.0.2rc1, 2.0.2rc2, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.1.0.dev1, 2.1.0.dev2, 2.1.0.dev3, 2.1.0.dev4, 2.1.0.dev5, 2.1.0.dev6, 2.1.0.dev7, 2.1.0.dev8, 2.1.0.dev9, 2.1.0.dev10, 2.1.0.dev11, 2.1.0.dev12, 2.1.0.dev13, 2.1.0.dev14, 2.1.0.dev15, 2.1.0.dev16, 2.1.0.dev17, 2.1.0.dev18, 2.1.0.dev19, 2.1.0.dev20, 2.1.0.dev21, 2.1.0.dev22, 2.1.0.dev23, 2.1.0.dev24, 2.1.0.dev25, 2.1.0.dev26, 2.1.0.dev27, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.2.0.dev1, 2.2.0.dev2, 2.2.0.dev3, 2.2.0.dev4, 2.2.0.dev5, 2.2.1, 2.2.2, 2.2.3, 2.3.0.dev0, 2.3.0.dev1, 2.3.0.dev2, 2.3.0, 2.4.0.dev0, 2.4.0.dev1, 2.4.0.dev2, 2.4.0.dev3)\n",
      "ERROR: No matching distribution found for omegaconf==2.0.6\n"
     ]
    }
   ],
   "source": [
    "!conda create -n llama-omni python=3.10\n",
    "!conda activate llama-omni\n",
    "!pip install pip==24.0\n",
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: 'fairseq'\n",
      "f:\\AI\\Kì 5\\DeepLearning\\Proj\\LLaMA-Omni\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'fairseq'...\n",
      "error: RPC failed; curl 18 Transferred a partial file\n",
      "error: 2756 bytes of body are still expected\n",
      "fetch-pack: unexpected disconnect while reading sideband packet\n",
      "fatal: early EOF\n",
      "fatal: fetch-pack: invalid index-pack output\n",
      "c:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\IPython\\core\\magics\\osm.py:393: UserWarning: using bookmarks requires you to install the `pickleshare` library.\n",
      "  bkms = self.shell.db.get('bookmarks', {})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///F:/AI/K%C3%AC%205/DeepLearning/Proj/LLaMA-Omni\n",
      "  Checking if build backend supports build_editable: started\n",
      "  Checking if build backend supports build_editable: finished with status 'done'\n",
      "  Preparing editable metadata (pyproject.toml): started\n",
      "  Preparing editable metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting torch==2.1.2 (from llama-omni==1.0.0)\n",
      "  Using cached torch-2.1.2-cp311-cp311-win_amd64.whl.metadata (26 kB)\n",
      "Collecting torchvision==0.16.2 (from llama-omni==1.0.0)\n",
      "  Using cached torchvision-0.16.2-cp311-cp311-win_amd64.whl.metadata (6.6 kB)\n",
      "Collecting torchaudio==2.1.2 (from llama-omni==1.0.0)\n",
      "  Using cached torchaudio-2.1.2-cp311-cp311-win_amd64.whl.metadata (6.4 kB)\n",
      "Collecting transformers==4.43.4 (from llama-omni==1.0.0)\n",
      "  Using cached transformers-4.43.4-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting tokenizers==0.19.1 (from llama-omni==1.0.0)\n",
      "  Using cached tokenizers-0.19.1-cp311-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting sentencepiece==0.1.99 (from llama-omni==1.0.0)\n",
      "  Using cached sentencepiece-0.1.99-cp311-cp311-win_amd64.whl.metadata (8.3 kB)\n",
      "Collecting shortuuid (from llama-omni==1.0.0)\n",
      "  Using cached shortuuid-1.0.13-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting accelerate==0.33.0 (from llama-omni==1.0.0)\n",
      "  Using cached accelerate-0.33.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting peft==0.11.1 (from llama-omni==1.0.0)\n",
      "  Using cached peft-0.11.1-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting bitsandbytes==0.43.1 (from llama-omni==1.0.0)\n",
      "  Using cached bitsandbytes-0.43.1-py3-none-win_amd64.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pydantic in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-omni==1.0.0) (2.7.4)\n",
      "Collecting markdown2[all] (from llama-omni==1.0.0)\n",
      "  Using cached markdown2-2.5.0-py2.py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-omni==1.0.0) (1.26.3)\n",
      "Collecting scikit-learn==1.2.2 (from llama-omni==1.0.0)\n",
      "  Using cached scikit_learn-1.2.2-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting gradio==4.43.0 (from llama-omni==1.0.0)\n",
      "  Using cached gradio-4.43.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting gradio-client==1.3.0 (from llama-omni==1.0.0)\n",
      "  Using cached gradio_client-1.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\admin\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from llama-omni==1.0.0) (2.32.3)\n",
      "Collecting httpx==0.27.2 (from llama-omni==1.0.0)\n",
      "  Using cached httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting uvicorn (from llama-omni==1.0.0)\n",
      "  Using cached uvicorn-0.31.0-py3-none-any.whl.metadata (6.6 kB)\n",
      "Collecting fastapi (from llama-omni==1.0.0)\n",
      "  Using cached fastapi-0.115.0-py3-none-any.whl.metadata (27 kB)\n",
      "Collecting soundfile (from llama-omni==1.0.0)\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl.metadata (14 kB)\n",
      "Collecting einops==0.6.1 (from llama-omni==1.0.0)\n",
      "  Using cached einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting einops-exts==0.0.4 (from llama-omni==1.0.0)\n",
      "  Using cached einops_exts-0.0.4-py3-none-any.whl.metadata (621 bytes)\n",
      "Collecting timm==0.6.13 (from llama-omni==1.0.0)\n",
      "  Using cached timm-0.6.13-py3-none-any.whl.metadata (38 kB)\n",
      "Collecting openai-whisper (from llama-omni==1.0.0)\n",
      "  Using cached openai-whisper-20240930.tar.gz (800 kB)\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting setuptools==59.5.0 (from llama-omni==1.0.0)\n",
      "  Using cached setuptools-59.5.0-py3-none-any.whl.metadata (5.0 kB)\n",
      "Collecting omegaconf==2.0.6 (from llama-omni==1.0.0)\n",
      "  Using cached omegaconf-2.0.6-py3-none-any.whl.metadata (3.0 kB)\n",
      "INFO: pip is looking at multiple versions of llama-omni to determine which version is compatible with other requirements. This could take a while.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "  WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "WARNING: Ignoring version 2.0.6 of omegaconf since it has invalid metadata:\n",
      "Requested omegaconf==2.0.6 from https://files.pythonhosted.org/packages/d0/eb/9d63ce09dd8aa85767c65668d5414958ea29648a0eec80a4a7d311ec2684/omegaconf-2.0.6-py3-none-any.whl (from llama-omni==1.0.0) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
      "    PyYAML (>=5.1.*)\n",
      "            ~~~~~~^\n",
      "Please use pip<24.1 if you need to use this version.\n",
      "ERROR: Ignored the following yanked versions: 1.0.0, 1.0.1, 1.0.2, 2.0.0rc1, 2.0.0rc2, 2.0.0rc22, 2.0.0rc23, 2.0.0rc24, 2.0.0rc25, 2.0.0rc26, 2.0.0rc27, 2.0.0rc28, 2.0.0rc29, 2.0.1rc1, 2.0.1rc2, 2.0.1rc3, 2.0.1rc4, 2.0.1rc5, 2.2.0\n",
      "ERROR: Could not find a version that satisfies the requirement omegaconf==2.0.6 (from llama-omni) (from versions: 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.0.9, 1.0.10, 1.0.11, 1.0.12, 1.0.13, 1.0.14, 1.0.16, 1.0.17, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.1.9, 1.1.10, 1.2.0, 1.2.1, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0rc4, 1.3.0rc5, 1.3.0rc6, 1.3.0rc7, 1.3.0rc8, 1.3.0rc9, 1.3.0rc10, 1.3.0, 1.4.0rc1, 1.4.0rc2, 1.4.0rc3, 1.4.0rc4, 1.4.0, 1.4.1rc1, 1.4.1, 2.0.0rc3, 2.0.0rc4, 2.0.0rc5, 2.0.0rc6, 2.0.0rc7, 2.0.0rc8, 2.0.0rc9, 2.0.0rc10, 2.0.0rc11, 2.0.0rc12, 2.0.0rc13, 2.0.0rc14, 2.0.0rc15, 2.0.0rc16, 2.0.0rc17, 2.0.0rc18, 2.0.0rc19, 2.0.0rc20, 2.0.0rc21, 2.0.0, 2.0.1rc6, 2.0.1rc7, 2.0.1rc8, 2.0.1rc9, 2.0.1rc10, 2.0.1rc11, 2.0.1rc12, 2.0.1rc13, 2.0.1, 2.0.2rc1, 2.0.2rc2, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.1.0.dev1, 2.1.0.dev2, 2.1.0.dev3, 2.1.0.dev4, 2.1.0.dev5, 2.1.0.dev6, 2.1.0.dev7, 2.1.0.dev8, 2.1.0.dev9, 2.1.0.dev10, 2.1.0.dev11, 2.1.0.dev12, 2.1.0.dev13, 2.1.0.dev14, 2.1.0.dev15, 2.1.0.dev16, 2.1.0.dev17, 2.1.0.dev18, 2.1.0.dev19, 2.1.0.dev20, 2.1.0.dev21, 2.1.0.dev22, 2.1.0.dev23, 2.1.0.dev24, 2.1.0.dev25, 2.1.0.dev26, 2.1.0.dev27, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.2.0.dev1, 2.2.0.dev2, 2.2.0.dev3, 2.2.0.dev4, 2.2.0.dev5, 2.2.1, 2.2.2, 2.2.3, 2.3.0.dev0, 2.3.0.dev1, 2.3.0.dev2, 2.3.0, 2.4.0.dev0, 2.4.0.dev1, 2.4.0.dev2, 2.4.0.dev3)\n",
      "ERROR: No matching distribution found for omegaconf==2.0.6\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/pytorch/fairseq\n",
    "%cd fairseq\n",
    "!pip install -e . --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.6.3.tar.gz (2.6 MB)\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.3/2.6 MB ? eta -:--:--\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ------- -------------------------------- 0.5/2.6 MB 66.8 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     ----------- ---------------------------- 0.8/2.6 MB 58.7 kB/s eta 0:00:32\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     --------------- ------------------------ 1.0/2.6 MB 66.8 kB/s eta 0:00:24\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ------------------- -------------------- 1.3/2.6 MB 74.0 kB/s eta 0:00:18\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     ----------------------- ---------------- 1.6/2.6 MB 69.8 kB/s eta 0:00:16\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     --------------------------- ------------ 1.8/2.6 MB 66.4 kB/s eta 0:00:12\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ------------------------------- -------- 2.1/2.6 MB 58.7 kB/s eta 0:00:10\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ----------------------------------- ---- 2.4/2.6 MB 61.1 kB/s eta 0:00:05\n",
      "     ---------------------------------------  2.6/2.6 MB 62.8 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.6/2.6 MB 62.7 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution ~atplotlib (C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages)\n",
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [22 lines of output]\n",
      "      error: pathspec 'csrc/cutlass' did not match any file(s) known to git\n",
      "      \n",
      "      \n",
      "      torch.__version__  = 2.3.0+cpu\n",
      "      \n",
      "      \n",
      "      C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-2d2rbv6h\\flash-attn_18f5d6c099d640e2b18f75921d677840\\setup.py:95: UserWarning: flash_attn was requested, but nvcc was not found.  Are you sure your environment has nvcc available?  If you're installing within a container from https://hub.docker.com/r/pytorch/pytorch, only images whose names contain 'devel' will provide nvcc.\n",
      "        warnings.warn(\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Temp\\pip-install-2d2rbv6h\\flash-attn_18f5d6c099d640e2b18f75921d677840\\setup.py\", line 179, in <module>\n",
      "          CUDAExtension(\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1077, in CUDAExtension\n",
      "          library_dirs += library_paths(cuda=True)\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 1211, in library_paths\n",
      "          paths.append(_join_cuda_home(lib_dir))\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "        File \"C:\\Users\\Admin\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\cpp_extension.py\", line 2419, in _join_cuda_home\n",
      "          raise OSError('CUDA_HOME environment variable is not set. '\n",
      "      OSError: CUDA_HOME environment variable is not set. Please set it to your CUDA install root.\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!huggingface-cli download ICTNLP/Llama-3.1-8B-Omni --local-dir ICTNLP/Llama-3.1-8B-Omni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj/g_00500000 -P vocoder/\n",
    "!wget https://dl.fbaipublicfiles.com/fairseq/speech_to_speech/vocoder/code_hifigan/mhubert_vp_en_es_fr_it3_400k_layer11_km1000_lj/config.json -P vocoder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m omni_speech.serve.controller --host 0.0.0.0 --port 10000"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
